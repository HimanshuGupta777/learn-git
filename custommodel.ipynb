{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3e0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json,os\n",
    "from io import StringIO\n",
    "from scripts.uploadFile import UploadFile\n",
    "from scripts.getProjectDetailsForUser import GetProjectDetailsForUser\n",
    "from scripts.getModelDetailsForUser import GetModelDetailsForUser\n",
    "\n",
    "\n",
    "from scripts.trainModel import trainModel_classification\n",
    "from scripts.predictData import predictModelData\n",
    "from scripts.reportEDA import GetReportDetails\n",
    "\n",
    "\n",
    "\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0e33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "userName='dinesh'\n",
    "\n",
    "modeljson={\n",
    "    \"modelName\": \"3122021\",\n",
    "    \"model\": \"None\",\n",
    "    \"sort_model\": \"F1\",\n",
    "    \"target\": \"Heart-Att\",\n",
    "    \"train_size\": \"0.70\",\n",
    "    \"categorical_imputation\": \"constant\",\n",
    "    \"numeric_imputation\": \"mean\",\n",
    "    \"normalize_method\": \"zscore\",\n",
    "    \"outliers_threshold\": \"None\",\n",
    "    \"multicollinearity_threshold\": \"None\"\n",
    "}\n",
    "\n",
    "\n",
    "thisProject= {'projectName': 'heart', 'createdOn': 20210321180903, 'datafileName': 'US_Heart_Patients.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a789dd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dinesh',\n",
       " 'heart',\n",
       " {'modelName': '3122021',\n",
       "  'model': 'None',\n",
       "  'sort_model': 'F1',\n",
       "  'target': 'Heart-Att',\n",
       "  'train_size': '0.70',\n",
       "  'categorical_imputation': 'constant',\n",
       "  'numeric_imputation': 'mean',\n",
       "  'normalize_method': 'zscore',\n",
       "  'outliers_threshold': 'None',\n",
       "  'multicollinearity_threshold': 'None'},\n",
       " 'US_Heart_Patients.csv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userName, thisProject[\"projectName\"],modeljson,thisProject[\"datafileName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5fef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -u USER -p PROJNAME [-f FILEPARAMETER]\n",
      "                             [-d DATAFILENAME]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -u/--user, -p/--projName\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import library\n",
    "from warnings import catch_warnings\n",
    "from numpy.lib.npyio import save\n",
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "import os, json, argparse\n",
    "from scripts.getProjectDetailsForUser import GetProjectDetailsForUser\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import imblearn\n",
    "import boto3\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "from boto3 import client\n",
    "\n",
    "class trainModel_classification():\n",
    "\n",
    "    def __init__(self, user: str, projName:str, fileParameter:str, dataFileName:str):\n",
    "        print('inside trainModel_classification init')\n",
    "        self.user = user\n",
    "        self.proj = projName\n",
    "        self.fileParameter = fileParameter\n",
    "        self.dataFileName = dataFileName\n",
    "\n",
    "        C = json.loads(fileParameter)\n",
    "        print(\"type after json\",type(C))\n",
    "        print(C)\n",
    "        #Reading csv file in dataframe\n",
    "        ProjectDetailsObj = GetProjectDetailsForUser(self.user, self.proj, self.dataFileName)\n",
    "        self.df = pd.read_csv(StringIO(ProjectDetailsObj.getProjectDataFile()))\n",
    "        print(\"After reading from another function\")        \n",
    "        print(self.df.columns.tolist())\n",
    "\n",
    "        #Reading values from dictionary\n",
    "        self.target_column =  C['target']\n",
    "        print(\"target_column: \", self.target_column)\n",
    "        self.training_size = C['train_size']\n",
    "        self.model_type = C['model']\n",
    "        self.category_imputation = C['categorical_imputation']\n",
    "        self.numerical_imputation = C['numeric_imputation']\n",
    "        self.normalization_method = C['normalize_method']\n",
    "        self.outliers_value = C['outliers_threshold']\n",
    "        self.multicollinearity_value = C['multicollinearity_threshold']\n",
    "        #setting up default values for parameters\n",
    "        #for categorical values\n",
    "        # if(self.category_imputation == \"None\"):\n",
    "        #     self.category_features = \"None\"\n",
    "        #     self.category_imputation = \"constant\"\n",
    "        # else :\n",
    "        #     self.category_features = C['categorical_features']\n",
    "\n",
    "        # #for numerical values\n",
    "        # if(self.numerical_imputation == \"None\"):\n",
    "        #     self.numerical_features = \"None\"\n",
    "        #     self.numerical_imputation = \"mean\"\n",
    "        # else :\n",
    "        #     self.numerical_features = C['numeric_features']\n",
    "\n",
    "        #for normalization\n",
    "        if(self.normalization_method == \"None\"):\n",
    "            self.normalization_type = \"False\"\n",
    "            self.normalization_method = \"zscore\"\n",
    "        else :\n",
    "            self.normalization_type = \"True\"\n",
    "            self.normalization_method = C['normalize_method']\n",
    "\n",
    "        #for Outliers\n",
    "        if(self.outliers_value == \"None\"):\n",
    "            self.outliers_type = \"False\"\n",
    "            self.outliers_value = \"0.05\"\n",
    "        else :\n",
    "            self.outliers_type = \"True\"\n",
    "            self.outliers_value = C['outliers_threshold']\n",
    "\n",
    "        #for multicollinearity\n",
    "        if(self.multicollinearity_value == \"None\"):\n",
    "            self.multicollinearity_type = \"False\"\n",
    "            self.multicollinearity_value = 0.9\n",
    "        else :\n",
    "            self.multicollinearity_type = \"True\"\n",
    "            self.multicollinearity_value = C['multicollinearity_threshold']\n",
    "        \n",
    "        self.sort_model = C['sort_model']\n",
    "        print(\"target_column: \", self.target_column)\n",
    "        print(\"training_size:\" , self.training_size )\n",
    "\n",
    "    # function to set values for model training\n",
    "    def modelTrain_Classification(self):\n",
    "        #setting up data for the model\n",
    "        # if self.category_imputation == 'None':\n",
    "        #     print(\"1st if Categorical is none\")\n",
    "        #     clf=setup(data=self.df,target=self.target_column,train_size = float(self.training_size),silent=True, \n",
    "        #     numeric_features= [self.numerical_features], numeric_imputation= self.numerical_imputation, normalize= bool(self.normalization_type),\n",
    "        #     normalize_method = self.normalization_method, remove_outliers= bool(self.outliers_type), \n",
    "        #     outliers_threshold = float(self.outliers_value), remove_multicollinearity = bool(self.multicollinearity_type), multicollinearity_threshold = float(self.multicollinearity_value))\n",
    "        # elif self.numerical_imputation == 'None':\n",
    "        #     print(\"2nd if numerical is none\")\n",
    "        #     clf=setup(data=self.df,target=self.target_column,train_size = float(self.training_size),silent=True,\n",
    "        #     categorical_features= [self.category_features],categorical_imputation= self.category_imputation,\n",
    "        #     normalize= bool(self.normalization_type), normalize_method = self.normalization_method, remove_outliers= bool(self.outliers_type), outliers_threshold = float(self.outliers_value), \n",
    "        #     remove_multicollinearity = bool(self.multicollinearity_type), multicollinearity_threshold = float(self.multicollinearity_value))\n",
    "        # elif self.category_imputation == 'None' and self.numerical_imputation == 'None':\n",
    "        #     print(\"3rd if Both are none\")\n",
    "        #     clf=setup(data=self.df,target=self.target_column,train_size = float(self.training_size),silent=True,\n",
    "        #     normalize= bool(self.normalization_type), normalize_method = self.normalization_method,\n",
    "        #     remove_outliers= bool(self.outliers_type), outliers_threshold = float(self.outliers_value), \n",
    "        #     remove_multicollinearity = bool(self.multicollinearity_type), multicollinearity_threshold = float(self.multicollinearity_value))\n",
    "        # else:\n",
    "        print(\"Automatically choosing\")\n",
    "        clf=setup(data=self.df,target=self.target_column,train_size = float(self.training_size),silent=True,\n",
    "        categorical_imputation= self.category_imputation,numeric_imputation= self.numerical_imputation,\n",
    "        normalize= bool(self.normalization_type), normalize_method = self.normalization_method,\n",
    "        remove_outliers= bool(self.outliers_type), outliers_threshold = float(self.outliers_value), \n",
    "        remove_multicollinearity = bool(self.multicollinearity_type), multicollinearity_threshold = float(self.multicollinearity_value)\n",
    "        ,fix_imbalance=True,fix_imbalance_method=imblearn.over_sampling.BorderlineSMOTE())\n",
    "        \n",
    "        # if self.model_type == 'None':\n",
    "        #     modelTrained = compare_models(sort = self.sort_model)\n",
    "        # else:\n",
    "        #     modelTrained = create_model(self.model_type)\n",
    "\n",
    "        global modelFinal\n",
    "        modelTrained = create_model('lr')\n",
    "        modelFinal = finalize_model(modelTrained)\n",
    "        print(\"model final\",modelFinal)\n",
    "        model_name = type(modelFinal).__name__\n",
    "        print(\"name of model\",model_name)\n",
    "        print(type(modelFinal))\n",
    "        global best_model_results\n",
    "        best_model_results = pull(modelFinal)\n",
    "        print(type(best_model_results))\n",
    "        # save_model(modelFinal, 'lr_26052021')\n",
    "        # best = tune_model(modelTrained, n_iter=200, choose_better=True)\n",
    "        # report the best model\n",
    "        # print(best)\n",
    "        try:\n",
    "            plot_model(modelFinal, save= True)\n",
    "            plot_model(modelFinal, save= True,plot = 'confusion_matrix')\n",
    "            os.replace(\"AUC.png\", \"static\\AUC.png\")\n",
    "            os.replace(\"Confusion Matrix.png\", \"static\\Confusion Matrix.png\")\n",
    "        except:\n",
    "            print(\"There is no AUC\")\n",
    "            return \"There is no AUC Plot in this Model\"\n",
    "        return best_model_results[:1]\n",
    "\n",
    "    # function to save model\n",
    "    def saveTrainedModel(fileParameter, userName, projectName):\n",
    "        C = json.loads(fileParameter)\n",
    "        print(\"Inside getProjectDeatilsForUser -- saveTrainedModel\")\n",
    "        print(\"best fit models\",best_model_results[:1])\n",
    "        bestFitModel = pd.DataFrame(best_model_results[:1])\n",
    "        print(\"printing best fit model\",best_model_results)\n",
    "        modelType = type(modelFinal).__name__\n",
    "        # print(\"Automatic selected model name\",model_type.str.split(\" \")[0])\n",
    "        # print(\"type of model\",type(model_type))\n",
    "        now = datetime.now()\n",
    "        currentDateTime = now.strftime(\"%m%d%Y%H%M%S\")\n",
    "        print(\"date and time \", currentDateTime)\n",
    "        userModelName = C['modelName']\n",
    "        # print(\"Model type \",model_type.str.replace(\" \", \"_\")[0])\n",
    "        # modelType = model_type.str.replace(\" \", \"_\")[0]\n",
    "        global saveModelName\n",
    "        saveModelName = userModelName +\"_\"+ modelType +\"_\"+ currentDateTime\n",
    "        print(\"Model Name in S3\",saveModelName)\n",
    "        save_model(modelFinal, saveModelName)\n",
    "        #file property\n",
    "        # dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        propertydictionary ={\n",
    "                            \"uploadedon\": currentDateTime ,\n",
    "                            \"model_file\": userModelName,\n",
    "                            \"model_type\": modelType,\n",
    "                            \"target_column\": C['target'],\n",
    "                            \"training_size\": C['train_size'],\n",
    "                            \"sort_model\": C['sort_model'],\n",
    "                            \"category_imputation\": C['categorical_imputation'],\n",
    "                            \"numerical_imputation\": C['numeric_imputation'],\n",
    "                            \"normalization_method\": C['normalize_method'],\n",
    "                            \"outliers_value\": C['outliers_threshold'],\n",
    "                            \"multicollinearity_value\": C['multicollinearity_threshold']\n",
    "                            }\n",
    "                        \n",
    "        # Serializing json  \n",
    "        with open(saveModelName+\".json\", 'w') as fp:\n",
    "            json.dump(propertydictionary, fp)\n",
    "        S3Conf = json.load(open('config.json'))\n",
    "        print(S3Conf)\n",
    "\n",
    "        # initialize s3 client\n",
    "        client = boto3.client('s3', aws_access_key_id=S3Conf['AWS_ACCESS_KEY'], aws_secret_access_key=S3Conf['AWS_ACCESS_SECRET_KEY'])\n",
    "        transfer = S3Transfer(client)\n",
    "        transfer.upload_file(saveModelName+\".pkl\", S3Conf['PROJECT_BUCKET'],\n",
    "                                 userName + \"/\" + projectName + \"/\" + \"model\" + \"/\" + saveModelName +\"/\"+ saveModelName +\".pkl\" )\n",
    "        transfer.upload_file(saveModelName+\".json\", S3Conf['PROJECT_BUCKET'],\n",
    "                                 userName + \"/\" + projectName + \"/\" + \"model\" + \"/\" + saveModelName +\"/\"+ saveModelName +\".json\" )\n",
    "        transfer.upload_file(\"static/AUC\"+\".png\", S3Conf['PROJECT_BUCKET'],\n",
    "                                 userName + \"/\" + projectName + \"/\" + \"model\" + \"/\" + saveModelName +\"/\"+ saveModelName +\".png\" )\n",
    "        \n",
    "        return saveModelName\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # contruct the argument parser and parse the arguments\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('-u', '--user', type=str, required=True, help='username')\n",
    "    ap.add_argument('-p', '--projName', type=str, required=True, help='project')\n",
    "    ap.add_argument('-f', '--fileParameter', type=str, help='parameter of the file to train')\n",
    "    ap.add_argument('-d', '--dataFileName', help='name of CSV file of data')\n",
    "    args = vars(ap.parse_args())\n",
    "    print(f'input arguments: {args}')\n",
    "    args_user = args['user']\n",
    "    args_projName = args['projName']\n",
    "    args_fileParameter = args['fileParameter']\n",
    "    args_dataFileName = args['dataFileName']\n",
    "    print(f'input user: {args_user}')\n",
    "    print(f'input proj: {args_projName}')\n",
    "    print(f'input fileparameter: {args_fileParameter}')\n",
    "    print(f'input datafile: {args_dataFileName}')\n",
    "\n",
    "    # invoke class\n",
    "    projectsDetails = trainModel_classification(args_user, args_projName, args_dataFileName , args_fileParameter)\n",
    "    projectsDetails.modelTrain_Classification()\n",
    "    print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d347fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside trainModel_classification init\n",
      "type after json <class 'dict'>\n",
      "{'modelName': '3122021', 'model': 'None', 'sort_model': 'F1', 'target': 'Heart-Att', 'train_size': '0.70', 'categorical_imputation': 'constant', 'numeric_imputation': 'mean', 'normalize_method': 'zscore', 'outliers_threshold': 'None', 'multicollinearity_threshold': 'None'}\n",
      "inside init\n",
      "inside getProjectDataFile\n",
      "After reading from another function\n",
      "['Gender', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BP Meds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'tot cholesterol', 'Systolic BP', 'Diastolic BP', 'BMI', 'heartRate', 'glucose', 'Heart-Att']\n",
      "target_column:  Heart-Att\n",
      "target_column:  Heart-Att\n",
      "training_size: 0.70\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-52a8a2f6389b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#trigger this trainModel_classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrainModelDetails\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainModel_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthisProject\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"projectName\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodeljson\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthisProject\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"datafileName\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msaveModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainModel_classification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveTrainedModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodeljson\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muserName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthisProject\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"projectName\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"function complete save model\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msaveModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0muserModelAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainModelDetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodelTrain_Classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\mlproject_modeltrain\\scripts\\trainModel.py\u001b[0m in \u001b[0;36msaveTrainedModel\u001b[1;34m(fileParameter, userName, projectName)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;31m# function to save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msaveTrainedModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileParameter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprojectName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileParameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Inside getProjectDeatilsForUser -- saveTrainedModel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best fit models\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_model_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env3.6\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             raise TypeError('the JSON object must be str, bytes or bytearray, '\n\u001b[1;32m--> 348\u001b[1;33m                             'not {!r}'.format(s.__class__.__name__))\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'surrogatepass'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not 'dict'"
     ]
    }
   ],
   "source": [
    "#trigger this trainModel_classification\n",
    "trainModelDetails = trainModel_classification(userName, thisProject[\"projectName\"],json.dumps(modeljson),thisProject[\"datafileName\"])\n",
    "saveModel = trainModel_classification.saveTrainedModel(modeljson,userName,thisProject[\"projectName\"])\n",
    "print(\"function complete save model\",saveModel)\n",
    "userModelAll = trainModelDetails.modelTrain_Classification()\n",
    "print(f'user Model details: {userModelAll}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b938ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'str'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(userModelAll).__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c865e2",
   "metadata": {},
   "source": [
    "\n",
    "1 train the model\n",
    "2 save model on local\n",
    "3 transfer model to s3\n",
    "\n",
    "4 download model from s3\n",
    "5 load the downloaded model\n",
    "6 read new data without label\n",
    "7 predic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58610b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686d12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 save model on local\n",
    "with open('model_pickle','wb') as f:\n",
    "    pickle.dump(trainModelDetails,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bb827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3', aws_access_key_id='AKIAX3DD5N2FRCWYDKNI', aws_secret_access_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5fffbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 transfer model to s3\n",
    "\n",
    "s3.upload_file('model_pickle', 'testbucketai', 'dinesh/heart/model_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05875b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 download model from s3 \n",
    "# result = s3.get_object(Bucket='testbucketai', Key= 'dinesh/heart/model_pickle.pkl')\n",
    "with open('model_pickle.pkl', 'wb') as f:\n",
    "                s3.download_fileobj('testbucketai','dinesh/heart/model_pickle.pkl', f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7678fd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.response.StreamingBody at 0x23ace49e2e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result['Body']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501dd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_df = pd.read_csv(file) \n",
    "# print(\"This is downloded csvDf from s3\",initial_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8831481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "# 5 load the downloaded model \n",
    "savedModel = load_model('model_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06ed8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scripts.trainModel.trainModel_classification at 0x19c45e24208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753fa86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  6 read new data without label\n",
    "data_unseen = pd.read_csv('demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02ddf1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Gender.1</th>\n",
       "      <th>Gender.2</th>\n",
       "      <th>Gender.3</th>\n",
       "      <th>Gender.4</th>\n",
       "      <th>Gender.5</th>\n",
       "      <th>Gender.6</th>\n",
       "      <th>Gender.7</th>\n",
       "      <th>Gender.8</th>\n",
       "      <th>Gender.9</th>\n",
       "      <th>Gender.10</th>\n",
       "      <th>Gender.11</th>\n",
       "      <th>Gender.12</th>\n",
       "      <th>Gender.13</th>\n",
       "      <th>Gender.14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>46.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Gender.1  Gender.2  Gender.3  Gender.4  Gender.5  Gender.6  \\\n",
       "0    Male      39.0       4.0       0.0       0.0       0.0         0   \n",
       "1  Female      46.0       2.0       0.0       0.0       0.0         0   \n",
       "2    Male      48.0       1.0       1.0      20.0       0.0         0   \n",
       "3  Female      61.0       3.0       1.0      30.0       0.0         0   \n",
       "4  Female      46.0       3.0       1.0      23.0       0.0         0   \n",
       "\n",
       "   Gender.7  Gender.8  Gender.9  Gender.10  Gender.11  Gender.12  Gender.13  \\\n",
       "0         0         0     195.0      106.0       70.0      26.97         80   \n",
       "1         0         0     250.0      121.0       81.0      28.73         95   \n",
       "2         0         0     245.0      127.5       80.0      25.34         75   \n",
       "3         1         0     225.0      150.0       95.0      28.58         65   \n",
       "4         0         0     285.0      130.0       84.0      23.10         85   \n",
       "\n",
       "   Gender.14  \n",
       "0       77.0  \n",
       "1       76.0  \n",
       "2       70.0  \n",
       "3      103.0  \n",
       "4       85.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unseen.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10709895",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'trainModel_classification' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7b49d9f19512>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 7 predic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msavedModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_unseen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\env3.6\\lib\\site-packages\\pycaret\\classification.py\u001b[0m in \u001b[0;36mpredict_model\u001b[1;34m(estimator, data, probability_threshold, encoded_labels, raw_score, round, verbose)\u001b[0m\n\u001b[0;32m   2011\u001b[0m         \u001b[0mround\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2013\u001b[1;33m         \u001b[0mml_usecase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMLUsecase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLASSIFICATION\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2014\u001b[0m     )\n\u001b[0;32m   2015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env3.6\\lib\\site-packages\\pycaret\\internal\\tabular.py\u001b[0m in \u001b[0;36mpredict_model\u001b[1;34m(estimator, data, probability_threshold, encoded_labels, raw_score, round, verbose, ml_usecase, display)\u001b[0m\n\u001b[0;32m   8747\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_estimator_from_meta_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8749\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8751\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env3.6\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'trainModel_classification' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# 7 predic\n",
    "predictions = predict_model(savedModel, data_unseen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
